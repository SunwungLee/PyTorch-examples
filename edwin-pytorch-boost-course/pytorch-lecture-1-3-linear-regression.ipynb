{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear regression\n",
    "# 1. Data definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hypothesis\n",
    "# 학습 데이터와 잘 맞는 직선을 찾자\n",
    "# y=Wx + b(weight+bias)\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "W = torch.zeros(1, requires_grad=True) # 이 두개 변수를 학습시키고 싶다.\n",
    "b = torch.zeros(1, requires_grad=True) # requires_grad = True\n",
    "hypothesis = x_train*W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. compute loss\n",
    "# 우리의 모델이 얼마나 정답과 가까운가 == cost, loss\n",
    "# Mean Squared Error(MSE)\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 W: 1.745, b: 0.579 Cost: 0.048403\n",
      "Epoch  200/1000 W: 1.800, b: 0.456 Cost: 0.029910\n",
      "Epoch  300/1000 W: 1.842, b: 0.358 Cost: 0.018483\n",
      "Epoch  400/1000 W: 1.876, b: 0.281 Cost: 0.011421\n",
      "Epoch  500/1000 W: 1.903, b: 0.221 Cost: 0.007058\n",
      "Epoch  600/1000 W: 1.923, b: 0.174 Cost: 0.004361\n",
      "Epoch  700/1000 W: 1.940, b: 0.137 Cost: 0.002695\n",
      "Epoch  800/1000 W: 1.953, b: 0.107 Cost: 0.001665\n",
      "Epoch  900/1000 W: 1.963, b: 0.084 Cost: 0.001029\n",
      "Epoch 1000/1000 W: 1.971, b: 0.066 Cost: 0.000636\n"
     ]
    }
   ],
   "source": [
    "# 4. Gradient descent\n",
    "optimizer = optim.SGD([W, b], lr=0.01) # 학습할 tensors, learning rate\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient 계산\n",
    "    optimizer.step() # 개선\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High-level Implementation with nn.Module\n",
    "# 이제 linear regression model을 만들게 되는데, 기본적으로 PyTorch의 모든 모델은 제공되는 nn.Module을 inherit해서 만들게된다.\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self): # 사용할 layer들을 정의, 우리는 linear regression model을 만들 것이므로 nn.linear를 이용.\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def forward(self, x): # 이 모델이 어떻게 입력값에서 출력값을 계산하는지 알려줌\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1650],\n",
      "        [0.3112],\n",
      "        [0.4575]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Hypothesis\n",
    "# 이제 모델을 생성해서 예측값 H(x)를 구하자.\n",
    "\n",
    "hypothesis = model(x_train)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1650],\n",
      "        [0.3112],\n",
      "        [0.4575]], grad_fn=<AddmmBackward>)\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "## Cost\n",
    "# 이제 MSE로 cost를 구하자. MSE 역시 PyTorch 에서 기본적으로 제공한다.\n",
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8978, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Descent\n",
    "# 마지막 주어진 cost를 이용해 H(x)의 W, b를 바꾸어서 cost를 줄여준다. 이때 PyTorch의 torch.optim에 있는 optimizer들 중 하나를 사용.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.270, b: -0.312 Cost: 34.129803\n",
      "Epoch  100/1000 W: 1.801, b: 0.452 Cost: 0.029475\n",
      "Epoch  200/1000 W: 1.844, b: 0.355 Cost: 0.018214\n",
      "Epoch  300/1000 W: 1.877, b: 0.279 Cost: 0.011255\n",
      "Epoch  400/1000 W: 1.903, b: 0.220 Cost: 0.006955\n",
      "Epoch  500/1000 W: 1.924, b: 0.173 Cost: 0.004298\n",
      "Epoch  600/1000 W: 1.940, b: 0.136 Cost: 0.002656\n",
      "Epoch  700/1000 W: 1.953, b: 0.107 Cost: 0.001641\n",
      "Epoch  800/1000 W: 1.963, b: 0.084 Cost: 0.001014\n",
      "Epoch  900/1000 W: 1.971, b: 0.066 Cost: 0.000627\n",
      "Epoch 1000/1000 W: 1.977, b: 0.052 Cost: 0.000387\n"
     ]
    }
   ],
   "source": [
    "## Training with Full Code\n",
    "# 이제 linear Regression 코드를 이해햇으니, 실제로 코드를 돌려 피팅시켜보자.\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 역전파 단계 전에, 갱신할 변수들에 ㅐ한 모든 변화도를 0으로 만들어야함. \n",
    "    # 기본적으로 .backward()를 할 때마다 변화도가 buffer에 누적되기 때문이다. torch.autograd.backward 문서 참조.\n",
    "    cost.backward() # 모델의 매개변수에 대한 손실의 변화도를 개선\n",
    "    optimizer.step() # 매개변수가 갱신.\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
