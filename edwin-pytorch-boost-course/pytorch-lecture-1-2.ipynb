{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# View == Reshape\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape) # == 총 12\n",
    "\n",
    "print(ft.view([-1, 3])) # 앞에는 모르겠고 3개의 차원을 가질래\n",
    "print(ft.view([-1, 3]).shape) # (2,2,3) --> (2x2, 3) == 총 12\n",
    "\n",
    "print(ft.view([-1, 1, 3])) # 앞은 모르겠고 XX, 1, 3 의 모양을 가질래\n",
    "print(ft.view([-1, 1, 3]).shape) # (2x2, 1, 3) == 총 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "---------------------\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n",
      "---------------------\n",
      "torch.Size([3])\n",
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Squeeze and Unsqueeze\n",
    "# Squeeze 쥐어 짜자. = dimension의 element가 1인 경우에 그 demension을 없애준다.\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "print('---------------------')\n",
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)\n",
    "print('---------------------')\n",
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.unsqueeze(0)) # the same as dim=0\n",
    "print(ft.unsqueeze(0).shape)\n",
    "\n",
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)\n",
    "\n",
    "print(ft.unsqueeze(-1)) # the same as dim=1\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "\n",
      "-------\n",
      "Casting\n",
      "-------\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n",
      "\n",
      "--------\n",
      "Stacking\n",
      "--------\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "--------\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([[0], [1], [2], [0]])\n",
    "print(lt)\n",
    "\n",
    "one_hot = torch.zeros(4, 3) # batch_size = 4, classes = 3\n",
    "one_hot.scatter_(1, lt, 1)\n",
    "print(one_hot)\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Casting')\n",
    "print('-------')\n",
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)\n",
    "print(lt.float())\n",
    "\n",
    "bt = torch.ByteTensor([True, False, False, True]) # boolean\n",
    "print(bt)\n",
    "print(bt.long())\n",
    "print(bt.float())\n",
    "\n",
    "print()\n",
    "print('--------')\n",
    "print('Stacking')\n",
    "print('--------')\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z], dim=1))\n",
    "print('--------')\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate --> Stacking으로 좀 더 편하게 사용 가능.\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(torch.cat([x, y], dim=0))\n",
    "print(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "---\n",
      "Zip\n",
      "---\n",
      "1 4\n",
      "2 5\n",
      "3 6\n",
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "# Ones and Zeros\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)\n",
    "print(torch.ones_like(x)) # x와 같은 shape의 행렬 생성\n",
    "print(torch.zeros_like(x))\n",
    "# ==> Device\n",
    "print()\n",
    "print('---')\n",
    "print('Zip')\n",
    "print('---')\n",
    "\n",
    "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
    "    print(x, y)\n",
    "\n",
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "---\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place Operation --> 속도 면에서 빠르지만, 크게 이점이 없을 수도 있다.\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(x.mul(2.))\n",
    "print(x)\n",
    "print('---')\n",
    "print(x.mul_(2.)) # x에 연산값 저장.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
